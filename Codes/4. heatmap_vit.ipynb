{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este c√≥digo se utiliza para procesar las im√°genes originales de los experimentos y obtener como resultado dos imagenes, \n",
    "en una de ellas se encuentran los mapas de calor por cabeza (son 12 cabezas que usa el transformer) y en la otra imagen\n",
    "encontramos el mapa de calor del promedio de la atenci√≥n del ViT en las 12 cabezas tambi√©n entrega la mediana de la atenci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from functools import partial\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gc\n",
    "import psutil\n",
    "import warnings\n",
    "from torch import Tensor\n",
    "from scipy.stats import mode\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  üìå Cargar imagenes\n",
    "\n",
    "Antes de definir el modelo, cargamos las im√°genes y realizamos preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Se cargaron 20 im√°genes correctamente.\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/imagenes_experimento_001\"\n",
    "\n",
    "# Obtener la lista de im√°genes disponibles\n",
    "image_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\".jpg\")]\n",
    "\n",
    "# Funci√≥n para preprocesar im√°genes a tensores\n",
    "def transform(img, img_size):\n",
    "    img = transforms.Resize(img_size)(img)\n",
    "    img = transforms.ToTensor()(img)\n",
    "    return img\n",
    "\n",
    "# Procesar todas las im√°genes y almacenarlas como tensores\n",
    "processed_images = []\n",
    "for img_path in image_files:\n",
    "    img = Image.open(img_path)\n",
    "    img_tensor = transform(img, (224, 224))  # Redimensionar a 224x224\n",
    "    processed_images.append(img_tensor)\n",
    "\n",
    "print(f\"[INFO] Se cargaron {len(processed_images)} im√°genes correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuraci√≥n del Entorno\n",
    "\n",
    "Configuraci√≥n del entorno para ejecutar el modelo en GPU (si est√° disponible) o en CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Usando dispositivo: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"[INFO] Usando dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definici√≥n de Funciones Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n de truncamiento normal para inicializaci√≥n de pesos\n",
    "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
    "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
    "\n",
    "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
    "    def norm_cdf(x):\n",
    "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
    "\n",
    "# Funci√≥n DropPath\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definici√≥n del Modelo Vision Transformer\n",
    "\n",
    "Se implementan los bloques principales del modelo ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C //\n",
    "                                  self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x, attn\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(\n",
    "            drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,\n",
    "                       act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        y, attn = self.attn(self.norm1(x))\n",
    "        if return_attention:\n",
    "            return attn\n",
    "        x = x + self.drop_path(y)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" \n",
    "    Image to Patch Embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim,\n",
    "                              kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, num_classes=0, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
    "                 drop_path_rate=0., norm_layer=nn.LayerNorm, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth decay rule\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Classifier head\n",
    "        self.head = nn.Linear(\n",
    "            embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def interpolate_pos_encoding(self, x, w, h):\n",
    "        npatch = x.shape[1] - 1\n",
    "        N = self.pos_embed.shape[1] - 1\n",
    "        if npatch == N and w == h:\n",
    "            return self.pos_embed\n",
    "        class_pos_embed = self.pos_embed[:, 0]\n",
    "        patch_pos_embed = self.pos_embed[:, 1:]\n",
    "        dim = x.shape[-1]\n",
    "        w0 = w // self.patch_embed.patch_size\n",
    "        h0 = h // self.patch_embed.patch_size\n",
    "        # we add a small number to avoid floating point error in the interpolation\n",
    "        # see discussion at https://github.com/facebookresearch/dino/issues/8\n",
    "        w0, h0 = w0 + 0.1, h0 + 0.1\n",
    "        patch_pos_embed = nn.functional.interpolate(\n",
    "            patch_pos_embed.reshape(1, int(math.sqrt(N)), int(\n",
    "                math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
    "            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n",
    "            mode='bicubic',\n",
    "        )\n",
    "        assert int(\n",
    "            w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]\n",
    "        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n",
    "\n",
    "    def prepare_tokens(self, x):\n",
    "        B, nc, w, h = x.shape\n",
    "        x = self.patch_embed(x)  # patch linear embedding\n",
    "\n",
    "        # add the [CLS] token to the embed patch tokens\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # add positional encoding to each token\n",
    "        x = x + self.interpolate_pos_encoding(x, w, h)\n",
    "\n",
    "        return self.pos_drop(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.prepare_tokens(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        return x[:, 0]\n",
    "\n",
    "    def get_last_selfattention(self, x):\n",
    "        x = self.prepare_tokens(x)\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            if i < len(self.blocks) - 1:\n",
    "                x = blk(x)\n",
    "            else:\n",
    "                # return attention of the last block\n",
    "                return blk(x, return_attention=True)\n",
    "\n",
    "    def get_intermediate_layers(self, x, n=1):\n",
    "        x = self.prepare_tokens(x)\n",
    "        # we return the output tokens from the `n` last blocks\n",
    "        output = []\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "            if len(self.blocks) - i <= n:\n",
    "                output.append(self.norm(x))\n",
    "        return output\n",
    "\n",
    "\n",
    "class VitGenerator(object):\n",
    "    def __init__(self, name_model, patch_size, device, evaluate=True, random=False, verbose=False):\n",
    "        self.name_model = name_model\n",
    "        self.patch_size = patch_size\n",
    "        self.evaluate = evaluate\n",
    "        self.device = device\n",
    "        self.verbose = verbose\n",
    "        self.model = self._getModel()\n",
    "        self._initializeModel()\n",
    "        if not random:\n",
    "            self._loadPretrainedWeights()\n",
    "\n",
    "    def _getModel(self):\n",
    "        if self.verbose:\n",
    "            print(\n",
    "                f\"[INFO] Initializing {self.name_model} with patch size of {self.patch_size}\")\n",
    "        if self.name_model == 'vit_tiny':\n",
    "            model = VisionTransformer(patch_size=self.patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,\n",
    "                                      qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
    "\n",
    "        elif self.name_model == 'vit_small':\n",
    "            model = VisionTransformer(patch_size=self.patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4,\n",
    "                                      qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
    "\n",
    "        elif self.name_model == 'vit_base':\n",
    "            model = VisionTransformer(patch_size=self.patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,\n",
    "                                      qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
    "        else:\n",
    "            raise f\"No model found with {self.name_model}\"\n",
    "\n",
    "        return model\n",
    "\n",
    "    def _initializeModel(self):\n",
    "        if self.evaluate:\n",
    "            for p in self.model.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            self.model.eval()\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def _loadPretrainedWeights(self):\n",
    "        if self.verbose:\n",
    "            print(\"[INFO] Loading weights\")\n",
    "        url = None\n",
    "        if self.name_model == 'vit_small' and self.patch_size == 16:\n",
    "            url = \"dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\"\n",
    "\n",
    "        elif self.name_model == 'vit_small' and self.patch_size == 8:\n",
    "            url = \"dino_deitsmall8_300ep_pretrain/dino_deitsmall8_300ep_pretrain.pth\"\n",
    "\n",
    "        elif self.name_model == 'vit_base' and self.patch_size == 16:\n",
    "            url = \"dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth\"\n",
    "\n",
    "        elif self.name_model == 'vit_base' and self.patch_size == 8:\n",
    "            url = \"dino_vitbase8_pretrain/dino_vitbase8_pretrain.pth\"\n",
    "\n",
    "        if url is None:\n",
    "            print(\n",
    "                f\"Since no pretrained weights have been found with name {self.name_model} and patch size {self.patch_size}, random weights will be used\")\n",
    "\n",
    "        else:\n",
    "            state_dict = torch.hub.load_state_dict_from_url(\n",
    "                url=\"https://dl.fbaipublicfiles.com/dino/\" + url)\n",
    "            self.model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    def get_last_selfattention(self, img):\n",
    "        return self.model.get_last_selfattention(img.to(self.device))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformaci√≥n y visualizaci√≥n de las imagenes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(img, img_size):\n",
    "    img = transforms.Resize(img_size)(img)\n",
    "    img = transforms.ToTensor()(img)\n",
    "    return img\n",
    "\n",
    "# Visualizaci√≥n de atenci√≥n\n",
    "def visualize_attention(model, img, patch_size, device):\n",
    "    w, h = img.shape[1] - img.shape[1] % patch_size, img.shape[2] - img.shape[2] % patch_size\n",
    "    img = img[:, :w, :h].unsqueeze(0)\n",
    "\n",
    "    w_featmap = img.shape[-2] // patch_size\n",
    "    h_featmap = img.shape[-1] // patch_size\n",
    "\n",
    "    attentions = model.get_last_selfattention(img.to(device))\n",
    "\n",
    "    nh = attentions.shape[1]\n",
    "    attentions = attentions[0, :, 0, 1:].reshape(nh, -1)\n",
    "    attentions = attentions.reshape(nh, w_featmap, h_featmap)\n",
    "    attentions = torch.nn.functional.interpolate(\n",
    "        attentions.unsqueeze(0), scale_factor=patch_size, mode=\"nearest\"\n",
    "    )[0].cpu().numpy()\n",
    "    return attentions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gr√°ficos de los m√≥dulos atenci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar gr√°ficos de atenci√≥n\n",
    "def plot_attention(img, attention, path):\n",
    "    n_heads = attention.shape[0]\n",
    "    \n",
    "    # Extraer el nombre de la carpeta basado en `path`\n",
    "    base_name = os.path.basename(path).split(\".\")[0]  # Por ejemplo, \"cesteria_01\"\n",
    "    \n",
    "    # Crear el directorio de salida correcto\n",
    "    output_dir = os.path.join(\n",
    "        \"C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\",\n",
    "        base_name,\n",
    "        \"csv\"\n",
    "    )\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"[INFO] Directorio de salida: {output_dir}\")\n",
    "\n",
    "    # **1. Visualizaci√≥n de la imagen original y la mediana de atenci√≥n**\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    attention_median = np.median(attention, axis=0)  # Calcular la mediana\n",
    "    plt.imshow(attention_median, cmap='jet')\n",
    "    plt.title(\"Attention Median\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Guardar la mediana como CSV\n",
    "    median_csv_path = os.path.join(output_dir, f\"{base_name}_attention_median.csv\")\n",
    "    np.savetxt(median_csv_path, attention_median, delimiter=',')\n",
    "    print(f\"[INFO] CSV de mediana guardado en: {median_csv_path}\")\n",
    "\n",
    "    # Guardar la visualizaci√≥n de la mediana como imagen\n",
    "    median_output_path = f\"{path}_attention_median.png\"\n",
    "    plt.savefig(median_output_path, bbox_inches='tight')\n",
    "    print(f\"[INFO] Imagen de mediana guardada en: {median_output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "    # **2. Visualizaci√≥n de la imagen original y el promedio de atenci√≥n**\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    attention_mean = np.mean(attention, axis=0)  # Calcular la media\n",
    "    plt.imshow(attention_mean, cmap='jet')\n",
    "    plt.title(\"Attention Mean\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Guardar el promedio como CSV\n",
    "    mean_csv_path = os.path.join(output_dir, f\"{base_name}_attention_mean.csv\")\n",
    "    np.savetxt(mean_csv_path, attention_mean, delimiter=',')\n",
    "    print(f\"[INFO] CSV de promedio guardado en: {mean_csv_path}\")\n",
    "\n",
    "    # Guardar la visualizaci√≥n del promedio como imagen\n",
    "    mean_output_path = f\"{path}_attention_mean.png\"\n",
    "    plt.savefig(mean_output_path, bbox_inches='tight')\n",
    "    print(f\"[INFO] Imagen de promedio guardada en: {mean_output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "    # **3. Visualizaci√≥n de los mapas de atenci√≥n por cabeza**\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(n_heads):\n",
    "        plt.subplot((n_heads + 2) // 3, 3, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.imshow(attention[i], cmap='jet', alpha=0.6)\n",
    "        plt.title(f\"Head {i+1}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Guardar cada mapa de atenci√≥n individual como CSV\n",
    "        head_csv_path = os.path.join(output_dir, f\"{base_name}_attention_h{i}.csv\")\n",
    "        np.savetxt(head_csv_path, attention[i], delimiter=',')\n",
    "        print(f\"[INFO] CSV de atenci√≥n de la cabeza {i+1} guardado en: {head_csv_path}\")\n",
    "\n",
    "    # Guardar la visualizaci√≥n de las cabezas como imagen\n",
    "    heads_output_path = f\"{path}_attention_heads.png\"\n",
    "    plt.savefig(heads_output_path, bbox_inches='tight')\n",
    "    print(f\"[INFO] Imagen de cabezas guardada en: {heads_output_path}\")\n",
    "    plt.close()\n",
    "    \n",
    "# Visualizaci√≥n de predicciones\n",
    "def visualize_predict(model, img, img_size, patch_size, device, path):\n",
    "    img_pre = transform(img, img_size)\n",
    "    attention = visualize_attention(model, img_pre, patch_size, device)\n",
    "    plot_attention(img, attention, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicializaci√≥n del dispositivo y modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initializing vit_base with patch size of 16\n",
      "[INFO] Loading weights\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.set_device(0)\n",
    "\n",
    "name_model = 'vit_base'\n",
    "patch_size = 16\n",
    "\n",
    "model = VitGenerator(name_model, patch_size, device, evaluate=True, random=False, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesar todas las im√°genes en la carpeta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## carpeta para guardar los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = r\"C:\\Users\\UsuarioCompuElite\\Desktop\\Tesis_doctorado\\Articulo_1\\metodologia\\Resultados_articulo1\\resultados_heatmap_ViT\"\n",
    "os.makedirs(output_folder, exist_ok=True)  # Crear carpeta si no existe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Procesamiento completo.\n"
     ]
    }
   ],
   "source": [
    "# Procesar una imagen a la vez\n",
    "def process_image(file_path):\n",
    "    img = Image.open(file_path)\n",
    "    try:\n",
    "        print(f\"[INFO] Procesando imagen: {os.path.basename(file_path)}\")\n",
    "        print(f\"[INFO] Memoria RAM utilizada antes: {psutil.virtual_memory().used / (1024 ** 3):.2f} GB\")\n",
    "\n",
    "        # Transformar la imagen a formato esperado (PIL -> Tensor)\n",
    "        img_size = tuple(np.array(img.size[::-1]))  # Tama√±o original de la imagen\n",
    "        img_tensor = transform(img, img_size)  # Transformar PIL -> Tensor\n",
    "\n",
    "        # Llamar a la funci√≥n de procesamiento con el modelo\n",
    "        output_base = os.path.splitext(file_path)[0]\n",
    "        visualize_predict(model, img, img_size, patch_size, device, output_base)\n",
    "\n",
    "        print(f\"[INFO] Memoria RAM utilizada despu√©s: {psutil.virtual_memory().used / (1024 ** 3):.2f} GB\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Error procesando la imagen {os.path.basename(file_path)}: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Liberar recursos asociados a la imagen\n",
    "        img.close()\n",
    "        del img\n",
    "        del img_tensor\n",
    "\n",
    "        # Liberar memoria de GPU y RAM\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        # Forzar recolecci√≥n de basura\n",
    "        gc.collect()\n",
    "\n",
    "# Procesar im√°genes una por una\n",
    "def main():\n",
    "    for filename in os.listdir(output_folder):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            file_path = os.path.join(output_folder, filename)\n",
    "            process_image(file_path)\n",
    "\n",
    "print(\"[INFO] Procesamiento completo.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcular la media y la desviaci√≥n est√°ndar para las distribuciones de los heatmap de cada imagen procesada por el ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error con basketry_01: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\basketry_01\\normalized_csv\\basketry_01_attention_mean_normalized.csv not found.\n",
      "‚ùå Error con basketry_02: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\basketry_02\\normalized_csv\\basketry_02_attention_mean_normalized.csv not found.\n",
      "‚ùå Error con basketry_03: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\basketry_03\\normalized_csv\\basketry_03_attention_mean_normalized.csv not found.\n",
      "‚ùå Error con basketry_04: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\basketry_04\\normalized_csv\\basketry_04_attention_mean_normalized.csv not found.\n",
      "‚ùå Error con basketry_05: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\basketry_05\\normalized_csv\\basketry_05_attention_mean_normalized.csv not found.\n",
      "‚ùå Error con basketry_06: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\basketry_06\\normalized_csv\\basketry_06_attention_mean_normalized.csv not found.\n",
      "‚ùå Error con basketry_07: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\basketry_07\\normalized_csv\\basketry_07_attention_mean_normalized.csv not found.\n",
      "‚ùå Error con basketry_08: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\basketry_08\\normalized_csv\\basketry_08_attention_mean_normalized.csv not found.\n",
      "‚ùå Error con basketry_09: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\basketry_09\\normalized_csv\\basketry_09_attention_mean_normalized.csv not found.\n",
      "‚ùå Error con basketry_10: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\basketry_10\\normalized_csv\\basketry_10_attention_mean_normalized.csv not found.\n",
      "‚ùå Error con jar_01: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\jar_01\\normalized_csv\\jar_01_attention_mean_normalized.csv not found.\n",
      "‚ùå Error con jar_02: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\jar_02\\normalized_csv\\jar_02_attention_mean_normalized.csv not found.\n",
      "‚ùå Error con jar_03: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\jar_03\\normalized_csv\\jar_03_attention_mean_normalized.csv not found.\n",
      "‚ùå Error con jar_04: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\jar_04\\normalized_csv\\jar_04_attention_mean_normalized.csv not found.\n",
      "‚ùå Error con jar_05: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\jar_05\\normalized_csv\\jar_05_attention_mean_normalized.csv not found.\n",
      "‚ùå Error con jar_06: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\jar_06\\normalized_csv\\jar_06_attention_mean_normalized.csv not found.\n",
      "‚ùå Error con jar_07: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\jar_07\\normalized_csv\\jar_07_attention_mean_normalized.csv not found.\n",
      "‚ùå Error con jar_08: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\jar_08\\normalized_csv\\jar_08_attention_mean_normalized.csv not found.\n",
      "‚ùå Error con jar_09: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\jar_09\\normalized_csv\\jar_09_attention_mean_normalized.csv not found.\n",
      "‚ùå Error con jar_10: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\jar_10\\normalized_csv\\jar_10_attention_mean_normalized.csv not found.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Imagen'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21052\\1568128731.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33m‚ùå Error con \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;31m# Crear DataFrame ordenado\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[0mdf_vit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestadisticas_vit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Imagen\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;31m# Mostrar tabla en consola\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"=\"\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m65\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\UsuarioCompuElite\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[0;32m   7185\u001b[0m             \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7186\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7187\u001b[0m             \u001b[1;31m# len(by) == 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7189\u001b[1;33m             \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7191\u001b[0m             \u001b[1;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7192\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\UsuarioCompuElite\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1908\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1910\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1911\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1913\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1914\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Imagen'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Lista de nombres de im√°genes\n",
    "imagenes = [\n",
    "    \"basketry_01\", \"basketry_02\", \"basketry_03\", \"basketry_04\", \"basketry_05\",\n",
    "    \"basketry_06\", \"basketry_07\", \"basketry_08\", \"basketry_09\", \"basketry_10\",\n",
    "    \"jar_01\", \"jar_02\", \"jar_03\", \"jar_04\", \"jar_05\",\n",
    "    \"jar_06\", \"jar_07\", \"jar_08\", \"jar_09\", \"jar_10\",\n",
    "]\n",
    "\n",
    "imagenes2 = [\n",
    "    \"cesteria_01\", \"cesteria_02\", \"cesteria_03\", \"cesteria_04\", \"cesteria_05\",\n",
    "    \"cesteria_06\", \"cesteria_07\", \"cesteria_08\", \"cesteria_09\", \"cesteria_10\",\n",
    "    \"jarra_01\", \"jarra_02\", \"jarra_03\", \"jarra_04\", \"jarra_05\",\n",
    "    \"jarra_06\", \"jarra_07\", \"jarra_08\", \"jarra_09\", \"jarra_10\"\n",
    "]\n",
    "\n",
    "# Ruta base donde est√°n los CSVs de atenci√≥n del ViT\n",
    "base_vit_path = r\"C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\"\n",
    "\n",
    "# Lista para guardar estad√≠sticas\n",
    "estadisticas_vit = []\n",
    "\n",
    "for img in imagenes:\n",
    "    try:\n",
    "        csv_path = os.path.join(base_vit_path, img, \"normalized_csv\", f\"{img}_attention_mean_normalized.csv\")\n",
    "        data = np.loadtxt(csv_path, delimiter=\",\")\n",
    "        media = np.mean(data)\n",
    "        desviacion = np.std(data)\n",
    "\n",
    "        estadisticas_vit.append({\n",
    "            \"Imagen\": img,\n",
    "            \"Media\": round(media, 6),\n",
    "            \"Desviaci√≥n est√°ndar\": round(desviacion, 6)\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error con {img}: {e}\")\n",
    "\n",
    "# Crear DataFrame ordenado\n",
    "df_vit = pd.DataFrame(estadisticas_vit).sort_values(\"Imagen\").reset_index(drop=True)\n",
    "\n",
    "# Mostrar tabla en consola\n",
    "print(\"\\n\" + \"=\"*65)\n",
    "print(\"         DISTRIBUCI√ìN_HEATMAP_ViT_ATTENTION_MEAN\")\n",
    "print(\"=\"*65)\n",
    "print(df_vit.to_string(index=False))\n",
    "\n",
    "# Guardar como CSV\n",
    "output_csv = os.path.join(base_vit_path, \"estadisticas_heatmaps_ViT.csv\")\n",
    "df_vit.to_csv(output_csv, index=False)\n",
    "print(f\"\\n‚úÖ CSV guardado en:\\n{output_csv}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
