{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código se utiliza para procesar las imágenes originales de los experimentos y obtener como resultado dos imagenes, \n",
    "en una de ellas se encuentran los mapas de calor por cabeza (son 12 cabezas que usa el transformer) y en la otra imagen\n",
    "encontramos el mapa de calor del promedio de la atención del ViT en las 12 cabezas también entrega la mediana de la atención"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from functools import partial\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gc\n",
    "import psutil\n",
    "import warnings\n",
    "from torch import Tensor\n",
    "from scipy.stats import mode\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  📌 Cargar imagenes\n",
    "\n",
    "Antes de definir el modelo, cargamos las imágenes y realizamos preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Se cargaron 20 imágenes correctamente.\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/imagenes_experimento_001\"\n",
    "\n",
    "# Obtener la lista de imágenes disponibles\n",
    "image_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\".jpg\")]\n",
    "\n",
    "# Función para preprocesar imágenes a tensores\n",
    "def transform(img, img_size):\n",
    "    img = transforms.Resize(img_size)(img)\n",
    "    img = transforms.ToTensor()(img)\n",
    "    return img\n",
    "\n",
    "# Procesar todas las imágenes y almacenarlas como tensores\n",
    "processed_images = []\n",
    "for img_path in image_files:\n",
    "    img = Image.open(img_path)\n",
    "    img_tensor = transform(img, (224, 224))  # Redimensionar a 224x224\n",
    "    processed_images.append(img_tensor)\n",
    "\n",
    "print(f\"[INFO] Se cargaron {len(processed_images)} imágenes correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuración del Entorno\n",
    "\n",
    "Configuración del entorno para ejecutar el modelo en GPU (si está disponible) o en CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Usando dispositivo: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"[INFO] Usando dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de Funciones Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de truncamiento normal para inicialización de pesos\n",
    "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
    "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
    "\n",
    "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
    "    def norm_cdf(x):\n",
    "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
    "\n",
    "# Función DropPath\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición del Modelo Vision Transformer\n",
    "\n",
    "Se implementan los bloques principales del modelo ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C //\n",
    "                                  self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x, attn\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(\n",
    "            drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,\n",
    "                       act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        y, attn = self.attn(self.norm1(x))\n",
    "        if return_attention:\n",
    "            return attn\n",
    "        x = x + self.drop_path(y)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" \n",
    "    Image to Patch Embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim,\n",
    "                              kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, num_classes=0, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
    "                 drop_path_rate=0., norm_layer=nn.LayerNorm, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth decay rule\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Classifier head\n",
    "        self.head = nn.Linear(\n",
    "            embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def interpolate_pos_encoding(self, x, w, h):\n",
    "        npatch = x.shape[1] - 1\n",
    "        N = self.pos_embed.shape[1] - 1\n",
    "        if npatch == N and w == h:\n",
    "            return self.pos_embed\n",
    "        class_pos_embed = self.pos_embed[:, 0]\n",
    "        patch_pos_embed = self.pos_embed[:, 1:]\n",
    "        dim = x.shape[-1]\n",
    "        w0 = w // self.patch_embed.patch_size\n",
    "        h0 = h // self.patch_embed.patch_size\n",
    "        # we add a small number to avoid floating point error in the interpolation\n",
    "        # see discussion at https://github.com/facebookresearch/dino/issues/8\n",
    "        w0, h0 = w0 + 0.1, h0 + 0.1\n",
    "        patch_pos_embed = nn.functional.interpolate(\n",
    "            patch_pos_embed.reshape(1, int(math.sqrt(N)), int(\n",
    "                math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
    "            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n",
    "            mode='bicubic',\n",
    "        )\n",
    "        assert int(\n",
    "            w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]\n",
    "        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n",
    "\n",
    "    def prepare_tokens(self, x):\n",
    "        B, nc, w, h = x.shape\n",
    "        x = self.patch_embed(x)  # patch linear embedding\n",
    "\n",
    "        # add the [CLS] token to the embed patch tokens\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # add positional encoding to each token\n",
    "        x = x + self.interpolate_pos_encoding(x, w, h)\n",
    "\n",
    "        return self.pos_drop(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.prepare_tokens(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        return x[:, 0]\n",
    "\n",
    "    def get_last_selfattention(self, x):\n",
    "        x = self.prepare_tokens(x)\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            if i < len(self.blocks) - 1:\n",
    "                x = blk(x)\n",
    "            else:\n",
    "                # return attention of the last block\n",
    "                return blk(x, return_attention=True)\n",
    "\n",
    "    def get_intermediate_layers(self, x, n=1):\n",
    "        x = self.prepare_tokens(x)\n",
    "        # we return the output tokens from the `n` last blocks\n",
    "        output = []\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "            if len(self.blocks) - i <= n:\n",
    "                output.append(self.norm(x))\n",
    "        return output\n",
    "\n",
    "\n",
    "class VitGenerator(object):\n",
    "    def __init__(self, name_model, patch_size, device, evaluate=True, random=False, verbose=False):\n",
    "        self.name_model = name_model\n",
    "        self.patch_size = patch_size\n",
    "        self.evaluate = evaluate\n",
    "        self.device = device\n",
    "        self.verbose = verbose\n",
    "        self.model = self._getModel()\n",
    "        self._initializeModel()\n",
    "        if not random:\n",
    "            self._loadPretrainedWeights()\n",
    "\n",
    "    def _getModel(self):\n",
    "        if self.verbose:\n",
    "            print(\n",
    "                f\"[INFO] Initializing {self.name_model} with patch size of {self.patch_size}\")\n",
    "        if self.name_model == 'vit_tiny':\n",
    "            model = VisionTransformer(patch_size=self.patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,\n",
    "                                      qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
    "\n",
    "        elif self.name_model == 'vit_small':\n",
    "            model = VisionTransformer(patch_size=self.patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4,\n",
    "                                      qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
    "\n",
    "        elif self.name_model == 'vit_base':\n",
    "            model = VisionTransformer(patch_size=self.patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,\n",
    "                                      qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
    "        else:\n",
    "            raise f\"No model found with {self.name_model}\"\n",
    "\n",
    "        return model\n",
    "\n",
    "    def _initializeModel(self):\n",
    "        if self.evaluate:\n",
    "            for p in self.model.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            self.model.eval()\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def _loadPretrainedWeights(self):\n",
    "        if self.verbose:\n",
    "            print(\"[INFO] Loading weights\")\n",
    "        url = None\n",
    "        if self.name_model == 'vit_small' and self.patch_size == 16:\n",
    "            url = \"dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\"\n",
    "\n",
    "        elif self.name_model == 'vit_small' and self.patch_size == 8:\n",
    "            url = \"dino_deitsmall8_300ep_pretrain/dino_deitsmall8_300ep_pretrain.pth\"\n",
    "\n",
    "        elif self.name_model == 'vit_base' and self.patch_size == 16:\n",
    "            url = \"dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth\"\n",
    "\n",
    "        elif self.name_model == 'vit_base' and self.patch_size == 8:\n",
    "            url = \"dino_vitbase8_pretrain/dino_vitbase8_pretrain.pth\"\n",
    "\n",
    "        if url is None:\n",
    "            print(\n",
    "                f\"Since no pretrained weights have been found with name {self.name_model} and patch size {self.patch_size}, random weights will be used\")\n",
    "\n",
    "        else:\n",
    "            state_dict = torch.hub.load_state_dict_from_url(\n",
    "                url=\"https://dl.fbaipublicfiles.com/dino/\" + url)\n",
    "            self.model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    def get_last_selfattention(self, img):\n",
    "        return self.model.get_last_selfattention(img.to(self.device))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformación y visualización de las imagenes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(img, img_size):\n",
    "    img = transforms.Resize(img_size)(img)\n",
    "    img = transforms.ToTensor()(img)\n",
    "    return img\n",
    "\n",
    "# Visualización de atención\n",
    "def visualize_attention(model, img, patch_size, device):\n",
    "    w, h = img.shape[1] - img.shape[1] % patch_size, img.shape[2] - img.shape[2] % patch_size\n",
    "    img = img[:, :w, :h].unsqueeze(0)\n",
    "\n",
    "    w_featmap = img.shape[-2] // patch_size\n",
    "    h_featmap = img.shape[-1] // patch_size\n",
    "\n",
    "    attentions = model.get_last_selfattention(img.to(device))\n",
    "\n",
    "    nh = attentions.shape[1]\n",
    "    attentions = attentions[0, :, 0, 1:].reshape(nh, -1)\n",
    "    attentions = attentions.reshape(nh, w_featmap, h_featmap)\n",
    "    attentions = torch.nn.functional.interpolate(\n",
    "        attentions.unsqueeze(0), scale_factor=patch_size, mode=\"nearest\"\n",
    "    )[0].cpu().numpy()\n",
    "    return attentions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gráficos de los módulos atención"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar gráficos de atención\n",
    "def plot_attention(img, attention, path):\n",
    "    n_heads = attention.shape[0]\n",
    "    \n",
    "    # Extraer el nombre de la carpeta basado en `path`\n",
    "    base_name = os.path.basename(path).split(\".\")[0]  # Por ejemplo, \"cesteria_01\"\n",
    "    \n",
    "    # Crear el directorio de salida correcto\n",
    "    output_dir = os.path.join(\n",
    "        \"C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\",\n",
    "        base_name,\n",
    "        \"csv\"\n",
    "    )\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"[INFO] Directorio de salida: {output_dir}\")\n",
    "\n",
    "    # **1. Visualización de la imagen original y la mediana de atención**\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    attention_median = np.median(attention, axis=0)  # Calcular la mediana\n",
    "    plt.imshow(attention_median, cmap='jet')\n",
    "    plt.title(\"Attention Median\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Guardar la mediana como CSV\n",
    "    median_csv_path = os.path.join(output_dir, f\"{base_name}_attention_median.csv\")\n",
    "    np.savetxt(median_csv_path, attention_median, delimiter=',')\n",
    "    print(f\"[INFO] CSV de mediana guardado en: {median_csv_path}\")\n",
    "\n",
    "    # Guardar la visualización de la mediana como imagen\n",
    "    median_output_path = f\"{path}_attention_median.png\"\n",
    "    plt.savefig(median_output_path, bbox_inches='tight')\n",
    "    print(f\"[INFO] Imagen de mediana guardada en: {median_output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "    # **2. Visualización de la imagen original y el promedio de atención**\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    attention_mean = np.mean(attention, axis=0)  # Calcular la media\n",
    "    plt.imshow(attention_mean, cmap='jet')\n",
    "    plt.title(\"Attention Mean\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Guardar el promedio como CSV\n",
    "    mean_csv_path = os.path.join(output_dir, f\"{base_name}_attention_mean.csv\")\n",
    "    np.savetxt(mean_csv_path, attention_mean, delimiter=',')\n",
    "    print(f\"[INFO] CSV de promedio guardado en: {mean_csv_path}\")\n",
    "\n",
    "    # Guardar la visualización del promedio como imagen\n",
    "    mean_output_path = f\"{path}_attention_mean.png\"\n",
    "    plt.savefig(mean_output_path, bbox_inches='tight')\n",
    "    print(f\"[INFO] Imagen de promedio guardada en: {mean_output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "    # **3. Visualización de los mapas de atención por cabeza**\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(n_heads):\n",
    "        plt.subplot((n_heads + 2) // 3, 3, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.imshow(attention[i], cmap='jet', alpha=0.6)\n",
    "        plt.title(f\"Head {i+1}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Guardar cada mapa de atención individual como CSV\n",
    "        head_csv_path = os.path.join(output_dir, f\"{base_name}_attention_h{i}.csv\")\n",
    "        np.savetxt(head_csv_path, attention[i], delimiter=',')\n",
    "        print(f\"[INFO] CSV de atención de la cabeza {i+1} guardado en: {head_csv_path}\")\n",
    "\n",
    "    # Guardar la visualización de las cabezas como imagen\n",
    "    heads_output_path = f\"{path}_attention_heads.png\"\n",
    "    plt.savefig(heads_output_path, bbox_inches='tight')\n",
    "    print(f\"[INFO] Imagen de cabezas guardada en: {heads_output_path}\")\n",
    "    plt.close()\n",
    "    \n",
    "# Visualización de predicciones\n",
    "def visualize_predict(model, img, img_size, patch_size, device, path):\n",
    "    img_pre = transform(img, img_size)\n",
    "    attention = visualize_attention(model, img_pre, patch_size, device)\n",
    "    plot_attention(img, attention, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicialización del dispositivo y modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initializing vit_base with patch size of 16\n",
      "[INFO] Loading weights\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.set_device(0)\n",
    "\n",
    "name_model = 'vit_base'\n",
    "patch_size = 16\n",
    "\n",
    "model = VitGenerator(name_model, patch_size, device, evaluate=True, random=False, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesar todas las imágenes en la carpeta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## carpeta para guardar los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = r\"C:\\Users\\UsuarioCompuElite\\Desktop\\Tesis_doctorado\\Articulo_1\\metodologia\\Resultados_articulo1\\resultados_heatmap_ViT\"\n",
    "os.makedirs(output_folder, exist_ok=True)  # Crear carpeta si no existe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Procesamiento completo.\n"
     ]
    }
   ],
   "source": [
    "# Procesar una imagen a la vez\n",
    "def process_image(file_path):\n",
    "    img = Image.open(file_path)\n",
    "    try:\n",
    "        print(f\"[INFO] Procesando imagen: {os.path.basename(file_path)}\")\n",
    "        print(f\"[INFO] Memoria RAM utilizada antes: {psutil.virtual_memory().used / (1024 ** 3):.2f} GB\")\n",
    "\n",
    "        # Transformar la imagen a formato esperado (PIL -> Tensor)\n",
    "        img_size = tuple(np.array(img.size[::-1]))  # Tamaño original de la imagen\n",
    "        img_tensor = transform(img, img_size)  # Transformar PIL -> Tensor\n",
    "\n",
    "        # Llamar a la función de procesamiento con el modelo\n",
    "        output_base = os.path.splitext(file_path)[0]\n",
    "        visualize_predict(model, img, img_size, patch_size, device, output_base)\n",
    "\n",
    "        print(f\"[INFO] Memoria RAM utilizada después: {psutil.virtual_memory().used / (1024 ** 3):.2f} GB\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Error procesando la imagen {os.path.basename(file_path)}: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Liberar recursos asociados a la imagen\n",
    "        img.close()\n",
    "        del img\n",
    "        del img_tensor\n",
    "\n",
    "        # Liberar memoria de GPU y RAM\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        # Forzar recolección de basura\n",
    "        gc.collect()\n",
    "\n",
    "# Procesar imágenes una por una\n",
    "def main():\n",
    "    for filename in os.listdir(output_folder):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            file_path = os.path.join(output_folder, filename)\n",
    "            process_image(file_path)\n",
    "\n",
    "print(\"[INFO] Procesamiento completo.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcular la media y la desviación estándar para las distribuciones de los heatmap de cada imagen procesada por el ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error con basketry_01: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\basketry_01\\normalized_csv\\basketry_01_attention_mean_normalized.csv not found.\n",
      "❌ Error con basketry_02: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\basketry_02\\normalized_csv\\basketry_02_attention_mean_normalized.csv not found.\n",
      "❌ Error con basketry_03: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\basketry_03\\normalized_csv\\basketry_03_attention_mean_normalized.csv not found.\n",
      "❌ Error con basketry_04: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\basketry_04\\normalized_csv\\basketry_04_attention_mean_normalized.csv not found.\n",
      "❌ Error con basketry_05: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\basketry_05\\normalized_csv\\basketry_05_attention_mean_normalized.csv not found.\n",
      "❌ Error con basketry_06: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\basketry_06\\normalized_csv\\basketry_06_attention_mean_normalized.csv not found.\n",
      "❌ Error con basketry_07: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\basketry_07\\normalized_csv\\basketry_07_attention_mean_normalized.csv not found.\n",
      "❌ Error con basketry_08: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\basketry_08\\normalized_csv\\basketry_08_attention_mean_normalized.csv not found.\n",
      "❌ Error con basketry_09: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\basketry_09\\normalized_csv\\basketry_09_attention_mean_normalized.csv not found.\n",
      "❌ Error con basketry_10: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\basketry_10\\normalized_csv\\basketry_10_attention_mean_normalized.csv not found.\n",
      "❌ Error con jar_01: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\jar_01\\normalized_csv\\jar_01_attention_mean_normalized.csv not found.\n",
      "❌ Error con jar_02: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\jar_02\\normalized_csv\\jar_02_attention_mean_normalized.csv not found.\n",
      "❌ Error con jar_03: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\jar_03\\normalized_csv\\jar_03_attention_mean_normalized.csv not found.\n",
      "❌ Error con jar_04: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\jar_04\\normalized_csv\\jar_04_attention_mean_normalized.csv not found.\n",
      "❌ Error con jar_05: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\jar_05\\normalized_csv\\jar_05_attention_mean_normalized.csv not found.\n",
      "❌ Error con jar_06: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\jar_06\\normalized_csv\\jar_06_attention_mean_normalized.csv not found.\n",
      "❌ Error con jar_07: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\jar_07\\normalized_csv\\jar_07_attention_mean_normalized.csv not found.\n",
      "❌ Error con jar_08: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\jar_08\\normalized_csv\\jar_08_attention_mean_normalized.csv not found.\n",
      "❌ Error con jar_09: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\jar_09\\normalized_csv\\jar_09_attention_mean_normalized.csv not found.\n",
      "❌ Error con jar_10: C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\\jar_10\\normalized_csv\\jar_10_attention_mean_normalized.csv not found.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Imagen'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21052\\1568128731.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33m❌ Error con \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;31m# Crear DataFrame ordenado\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[0mdf_vit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestadisticas_vit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Imagen\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;31m# Mostrar tabla en consola\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"=\"\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m65\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\UsuarioCompuElite\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[0;32m   7185\u001b[0m             \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7186\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7187\u001b[0m             \u001b[1;31m# len(by) == 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7189\u001b[1;33m             \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7191\u001b[0m             \u001b[1;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7192\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\UsuarioCompuElite\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1908\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1910\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1911\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1913\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1914\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Imagen'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Lista de nombres de imágenes\n",
    "imagenes = [\n",
    "    \"basketry_01\", \"basketry_02\", \"basketry_03\", \"basketry_04\", \"basketry_05\",\n",
    "    \"basketry_06\", \"basketry_07\", \"basketry_08\", \"basketry_09\", \"basketry_10\",\n",
    "    \"jar_01\", \"jar_02\", \"jar_03\", \"jar_04\", \"jar_05\",\n",
    "    \"jar_06\", \"jar_07\", \"jar_08\", \"jar_09\", \"jar_10\",\n",
    "]\n",
    "\n",
    "imagenes2 = [\n",
    "    \"cesteria_01\", \"cesteria_02\", \"cesteria_03\", \"cesteria_04\", \"cesteria_05\",\n",
    "    \"cesteria_06\", \"cesteria_07\", \"cesteria_08\", \"cesteria_09\", \"cesteria_10\",\n",
    "    \"jarra_01\", \"jarra_02\", \"jarra_03\", \"jarra_04\", \"jarra_05\",\n",
    "    \"jarra_06\", \"jarra_07\", \"jarra_08\", \"jarra_09\", \"jarra_10\"\n",
    "]\n",
    "\n",
    "# Ruta base donde están los CSVs de atención del ViT\n",
    "base_vit_path = r\"C:/Users/UsuarioCompuElite/Desktop/Tesis_doctorado/Articulo_1/metodologia/Resultados_vit_experimento_001\"\n",
    "\n",
    "# Lista para guardar estadísticas\n",
    "estadisticas_vit = []\n",
    "\n",
    "for img in imagenes:\n",
    "    try:\n",
    "        csv_path = os.path.join(base_vit_path, img, \"normalized_csv\", f\"{img}_attention_mean_normalized.csv\")\n",
    "        data = np.loadtxt(csv_path, delimiter=\",\")\n",
    "        media = np.mean(data)\n",
    "        desviacion = np.std(data)\n",
    "\n",
    "        estadisticas_vit.append({\n",
    "            \"Imagen\": img,\n",
    "            \"Media\": round(media, 6),\n",
    "            \"Desviación estándar\": round(desviacion, 6)\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error con {img}: {e}\")\n",
    "\n",
    "# Crear DataFrame ordenado\n",
    "df_vit = pd.DataFrame(estadisticas_vit).sort_values(\"Imagen\").reset_index(drop=True)\n",
    "\n",
    "# Mostrar tabla en consola\n",
    "print(\"\\n\" + \"=\"*65)\n",
    "print(\"         DISTRIBUCIÓN_HEATMAP_ViT_ATTENTION_MEAN\")\n",
    "print(\"=\"*65)\n",
    "print(df_vit.to_string(index=False))\n",
    "\n",
    "# Guardar como CSV\n",
    "output_csv = os.path.join(base_vit_path, \"estadisticas_heatmaps_ViT.csv\")\n",
    "df_vit.to_csv(output_csv, index=False)\n",
    "print(f\"\\n✅ CSV guardado en:\\n{output_csv}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
