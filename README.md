# Visual Attention and Vision Transformers

This repository contains the code and experimental results for analyzing and comparing human attention maps with the attention generated by Vision Transformer (ViT) models.
The goal is to evaluate similarities and differences between human visual attention and the learned attention in transformer-based models.

## 📂 Repository structure
### Codes/
Contains all scripts and notebooks required to run the experiment in the order specified below.

### experiments_pictures/
Contains the input images and the results generated during the experiments.

## ▶️ Execution order
To fully replicate the experiment, run the scripts and notebooks inside the Codes/ folder in the following order:

1. data separation by matrix.py
        Separates and organizes input data for subsequent analysis.

2.  heatmap_hum_gaussiano_step1.ipynb
        Generates Gaussian-based heatmaps from human attention data.

3. gmm_avg_human_att.py
        Computes the average human attention using Gaussian Mixture Models.

4. heatmap_vit_step2.ipynb
        Generates the Vision Transformer attention maps.

5. Visualizing_in_ViT_step2.py
        Processes and visualizes the ViT attention outputs.

6. graphs_cum_avg_median.py
        Creates cumulative graphs and calculates statistical measures such as mean and median.

7. Metrics_and_figures.py
        Computes comparison metrics and generates the final figures for the analysis.

## ⚙️ Requirements
Python 3.8 or higher

### Required libraries: 
        numpy pandas matplotlib seaborn scikit-learn torch torchvision tensorflow

## 🚀 How to run

### Clone the repository:
        1. git clone https://github.com/luis-oliveros/Visual_Attention_and_ViT.git
        2. cd Visual_Attention_and_ViT
        3. Install the dependencies listed in the Requirements section.

Run each script or notebook following the order in the Execution order section.

All generated results will be automatically saved in the experiments_pictures/ folder.

## 📊 Execution flow diagram
![Execution Flow](diagram.png)

## 📄 License
This project is licensed under the MIT License. See the LICENSE file for details.


